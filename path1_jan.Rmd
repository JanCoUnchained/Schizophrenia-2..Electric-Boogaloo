---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#packages
## LOAD IN PACKAGES

library(pacman)
p_load(tidyverse, 
       knitr, #kable()
       lmerTest, #lmer()
       caret, #sensitivity() etc.
       MASS, #stepAIC()
       pROC, #ci()
       boot, #inv.logit()
       groupdata2, #fold()
       reshape2,
       buildmer, #black-box
       MuMIn)  #dredge()

select <- dplyr::select

## LOAD IN DATA

data <- read.csv("data.csv") %>%
  mutate(ID = factor(ID),
         study = factor(study),
         uniqueness = paste0(ID, diagnosis),
         diagnosis = factor(diagnosis))

clinic <- read.delim("A3_Clinic_Info.txt") %>%
  rename(ID = Subject, 
         study = Study) %>%
  mutate(ID = factor(ID),
         study = factor(study),
         diagnosis = ifelse(Diagnosis == "Control", 0, 1),
         uniqueness = paste0(ID, diagnosis)) %>%
  dplyr::select(ID, study, Gender, uniqueness)


## MERGE
# left join = keeping everyone who appears in "data". Removing those who are in "clinic" only.
# see https://docs.google.com/document/d/1csAffjmezQZHDmsOKzjSiGkD03BWxdDiJLSVOTRC98o/edit
Bear <- inner_join(data, clinic, by = c("uniqueness")) %>%
  dplyr::select(-ID.y, -study.y) %>%
  rename(ID = ID.x, study = study.x) %>%
  mutate(uniqueness = as.numeric(uniqueness))


## CLEAN UP AFTER YOURSELVES
rm(data)
rm(clinic)


## s33d
set.seed(1337)
```



### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.

Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?


```{r scaling}
# new df not to overwrite
Bear_scaled <- Bear

# MINMAX
for (i in c(6:13)) {
  minc = min(Bear_scaled[,i])
  maxc = max(Bear_scaled[,i])
  Bear_scaled[,i] = (Bear_scaled[,i]-minc)/(maxc-minc)
}

# keep it clean
rm(i, maxc, minc)
```




# Q1: range as predictor
Training model. Range only

```{r range model}
## range model
model_range <- glm(diagnosis ~ range, Bear_scaled, family= "binomial")


## PREDICT
# predicting response 
range_pred <- inv.logit(predict(model_range, Bear_scaled, allow.new.levels = TRUE))
range_pred_num <- inv.logit(predict(model_range, Bear_scaled, allow.new.levels = TRUE))

# turning it into 2 level factor
range_pred[range_pred < 0.5] = "0"
range_pred[range_pred >= 0.5] = "1"
```

## ANSWER 1
tidyer output later on (id_uniq_perf)
```{r range performance}
## SENSITIVITY SPECIFICITY
r_sen <- caret::sensitivity(factor(range_pred), factor(Bear_scaled$diagnosis), positive = "1")
r_spe <- caret::specificity(factor(range_pred), factor(Bear_scaled$diagnosis), negative = "0")

## PPV / NPV
r_ppv <- posPredValue(factor(range_pred), factor(Bear_scaled$diagnosis), positive = "1")
r_npv <- negPredValue(factor(range_pred), factor(Bear_scaled$diagnosis), negative = "0")

## ACCURACY
# table to asses
table_range <- table(Bear_scaled$diagnosis, range_pred)

# accuracy function
acc_fun <- function(table) {
  acc = (table[1]+table[4])/sum(table)
  return(round(acc, 2))
}

# accuracy score
r_acc <- acc_fun(table_range)


#ROC curve 
rocCurve <- roc(response = Bear_scaled$diagnosis, 
                predictor = range_pred_num)
r_auc <- pROC::auc(rocCurve)

ci(rocCurve)
plot(rocCurve, legacy.axes = TRUE)


## OUTPUT
range_no <- (cbind.data.frame('model' = "non cross-validated",
           'SEN' = r_sen,
           'SPE' = r_spe,
           'PPV' = r_ppv,
           'NPV' = r_npv,
           'AUC' = r_auc,
           'ACC' = r_acc))

rm(model_range, rocCurve, table_range)
rm(range_pred, range_pred_num)
rm(r_sen, r_spe, r_ppv, r_npv, r_auc, r_acc)
```



# CROSSVALIDATION
-creating folds
```{r folds}
set.seed(1337)
# adds a new column with fold number
Bear_scaled <- fold(Bear_scaled, k = 5, 
             cat_col = 'diagnosis', #balances ratio between shizo/control
             id_col = 'ID') #keeps same people in same folds

```


-define funciton
```{r loop}
# heavily inspired by Ludvig Renbo Olsen
# https://cran.r-project.org/web/packages/groupdata2/vignettes/cross-validation_with_groupdata2.html

crossvalidation_nation <- function(data, k, model_name, dependent, pos, neg, random = TRUE) {
  # Initialize empty list for recording performances
  performances_SEN <- c()
  performances_SPE <- c()
  performances_PPV <- c()
  performances_NPV <- c()
  performances_AUC <- c()
  performances_ACC <- c()

  
    # One iteration per fold
  for (fold in 1:k){   #from 1 to k
    
    # Create training set for this iteration
    # Subset all the datapoints where .folds does not match the current fold
    training_set <- data[data$.folds != fold,]
    
    # Create test set for this iteration
    # Subset all the datapoints where .folds matches the current fold
    testing_set <- data[data$.folds == fold,]
    
    ## Train model

    # If there is a random effect,
    # use lmer() to train model
    # else use lm()
    
     if (isTRUE(random)){

      model <- glmer(model_name, training_set, family = "binomial")

    } else {
      
      model <- glm(model_name, training_set, family = "binomial")

    }

    


    ## Test model

    # Predict the dependent variable in the testing_set with the trained model
    predicted <- inv.logit(predict(model, testing_set, allow.new.levels=TRUE))
    
    # Make predicted into factors
    predicted[predicted < 0.5] = 0
    predicted[predicted >= 0.5] = 1
    predicted_f <- factor(predicted)
    
    # Get model performance metrics between the predicted and the observed
    SEN <- caret::sensitivity(predicted_f, testing_set[[dependent]], positive = pos)
    
    SPE <- caret::specificity(predicted_f, testing_set[[dependent]], negative = neg)
    
    PPV <- posPredValue(predicted_f, testing_set[[dependent]], positive = pos)
    
    NPV <- negPredValue(predicted_f, testing_set[[dependent]], negative = neg)
    
    rocCurve <- roc(response = testing_set$diagnosis, predictor = predicted)
    AUC <- pROC::auc(rocCurve)
    
    testing_set$predicted <- predicted_f
    testing_correct <- filter(testing_set, diagnosis == as.character(predicted_f))
    testing_set <- testing_set
    ACC <- nrow(testing_correct) / nrow(testing_set)
    
    # Add the to the performance list
    performances_SEN[fold] <- SEN
    performances_SPE[fold] <- SPE
    performances_PPV[fold] <- PPV
    performances_NPV[fold] <- NPV
    performances_AUC[fold] <- AUC
    performances_ACC[fold] <- ACC

  }
  
  se <- function(x, na.rm = T) sqrt(var(x)/length(x))

  # Return the mean of the recorded RMSEs
  return(cbind.data.frame('model' = model_name,
           'SEN' = mean(performances_SEN, na.rm = T),
           'SPE' = mean(performances_SPE, na.rm = T),
           'PPV' = mean(performances_PPV, na.rm = T),
           'NPV' = mean(performances_NPV, na.rm = T),
           'AUC' = mean(performances_AUC, na.rm = T),
           'ACC' = mean(performances_ACC, na.rm = T),
           'SEN_SE' = se(performances_SEN, na.rm = T),
           'SPE_SE' = se(performances_SPE, na.rm = T),
           'PPV_SE' = se(performances_PPV, na.rm = T),
           'NPV_SE' = se(performances_NPV, na.rm = T),
           'AUC_SE' = se(performances_AUC, na.rm = T),
           'ACC_SE' = se(performances_ACC, na.rm = T)))

}
```


## cross-validating the range only model
```{r}
range_yes <- crossvalidation_nation(Bear_scaled, 
                       k = 5,
                       model_name = "diagnosis ~ range",
                       dependent = "diagnosis", 
                       pos = "1", 
                       neg = "0", 
                       random = FALSE)

range_yes$model <- as.character(range_yes$model)
range_yes[1,1] <- "cross-validated"
```

Crossvalidation doesn't compromise the model!
```{r}
range_comp <- range_yes %>%
  full_join(range_no) 

  range_comp[,2:7] <- round(range_comp[,2:7], 3)


rm(range_no, range_yes)
```

Range comparisson
```{r}
range_models <- mean_par %>%
  filter(str_detect(model, "~ range")) %>%
  filter(!str_detect(model, "Gender")) %>%
  filter(!str_detect(model, "iqr")) %>%
  filter(!str_detect(model, "mean")) %>%
  filter(!str_detect(model, "median")) %>%
  filter(!str_detect(model, "se")) %>%
  filter(!str_detect(model, "coef_var")) %>%
  arrange(desc(SEN))

  range_models[,2:7] <- round(range_models[,2:7], 3)
  
kable(range_models[,1:7])
```


<OFFTOPIC>
## ID vs uniqueness debate
Reasoning behind diagnosis ~ range + Gender + (1|ID) + (1|study)

Including only study as random effect increases performance quite a bit

```{r}
set.seed(1337)

# minimalistic
rm1 <- 'diagnosis ~ range + (1|study)'
rm2 <- 'diagnosis ~ range + (1|ID)'

# min + gender
rm3 <- 'diagnosis ~ range + Gender + (1|study)'
rm4 <- 'diagnosis ~ range + Gender + (1|ID)'

# combined / combined + gender
rm5 <- 'diagnosis ~ range + (1|ID) + (1|study)'
rm6 <- 'diagnosis ~ range + (1|Gender) + (1|ID) + (1|study)'


id_uniq_list <- list(rm1, rm2, rm3, rm4, rm5, rm6)

id_uniq_perf <- map_df(
            #list to loop trough
            id_uniq_list,
            #function to use
            crossvalidation_nation,
            # arguments
            data = Bear_scaled, 
            k = 5,
            dependent = "diagnosis", 
            pos = "1", 
            neg = "0")
```
</OFFTOPIC>



### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
# model list

```{r model list}
# creating model calls and putting them into list for later use
c_gis1 <- "diagnosis ~ range + Gender + (1|ID) + (1|study)"
c_gis2 <- "diagnosis ~ sd + Gender + (1|ID) + (1|study)"
c_gis3 <- "diagnosis ~ mean + Gender + (1|ID) + (1|study)"
c_gis4 <- "diagnosis ~ median + Gender + (1|ID) + (1|study)"
c_gis5 <- "diagnosis ~ iqr + Gender + (1|ID) + (1|study)"
c_gis6 <- "diagnosis ~ mean_abs + Gender + (1|ID) + (1|study)"
c_gis7 <- "diagnosis ~ coef_var + Gender + (1|ID) + (1|study)"

c_i1 <- "diagnosis ~ range + (1|ID)"
c_i2 <- "diagnosis ~ sd + (1|ID)"
c_i3 <- "diagnosis ~ mean + (1|ID)"
c_i4 <- "diagnosis ~ median + (1|ID)"
c_i5 <- "diagnosis ~ iqr + (1|ID)"
c_i6 <- "diagnosis ~ mean_abs + (1|ID)"
c_i7 <- "diagnosis ~ coef_var + (1|ID)"

c_gs1 <- "diagnosis ~ range + Gender +  (1|study)"
c_gs2 <- "diagnosis ~ sd + Gender +  (1|study)"
c_gs3 <- "diagnosis ~ mean + Gender +  (1|study)"
c_gs4 <- "diagnosis ~ median + Gender +  (1|study)"
c_gs5 <- "diagnosis ~ iqr + Gender +  (1|study)"
c_gs6 <- "diagnosis ~ mean_abs + Gender +  (1|study)"
c_gs7 <- "diagnosis ~ coef_var + Gender +  (1|study)"

c_s1 <- "diagnosis ~ range + (1|study)"
c_s2 <- "diagnosis ~ sd + (1|study)"
c_s3 <- "diagnosis ~ mean + (1|study)"
c_s4 <- "diagnosis ~ median + (1|study)"
c_s5 <- "diagnosis ~ iqr + (1|study)"
c_s6 <- "diagnosis ~ mean_abs + (1|study)"
c_s7 <- "diagnosis ~ coef_var + (1|study)"

c_n1 <- "diagnosis ~ range"
c_n2 <- "diagnosis ~ sd"
c_n3 <- "diagnosis ~ mean"
c_n4 <- "diagnosis ~ median"
c_n5 <- "diagnosis ~ iqr"
c_n6 <- "diagnosis ~ mean_abs"
c_n7 <- "diagnosis ~ coef_var"

model_list <- list(c_gis1, c_gis2, c_gis3, c_gis4, c_gis5, c_gis6, c_gis7,
                   c_i1, c_i2, c_i3, c_i4, c_i4, c_i5, c_i6, c_i7,
                   c_gs1, c_gs2, c_gs3, c_gs4, c_gs5, c_gs6, c_gs7,
                   c_s1, c_s2, c_s3, c_s4, c_s5, c_s6, c_s7)

no_random <- list(c_n1, c_n2, c_n3, c_n4, c_n5, c_n6, c_n7)

rm(list=ls(pattern="c_"))
```


### cross-validation loop

```{r output}
set.seed(1337)
cross_single <- map_df(
            #list to loop trough
            model_list,
            #function to use
            crossvalidation_nation,
            # arguments
            data = Bear_scaled, 
            k = 5,
            dependent = "diagnosis", 
            pos = "1", 
            neg = "0")

cross_single_norand <- map_df(
  #list to loop trough
            no_random,
            #function to use
            crossvalidation_nation,
            # arguments
            data = Bear_scaled, 
            k = 5,
            dependent = "diagnosis", 
            pos = "1", 
            neg = "0",
            random = FALSE)

rm(model_list, no_random)
```


### best predictor
Median seems to be the best predictor. It scores the highest on every performance meassure except for accuracy.
```{r}
cross_single_all <- full_join(cross_single_norand, cross_single)

mean_single <- cross_single_all %>%
  rowwise() %>%
  mutate(pos_mean = mean(c(SEN, PPV)),
         pos_error = mean(c(SEN_SE, PPV_SE)),
         neg_mean = mean(c(SPE, NPV)),
         neg_error = mean(c(SPE_SE, NPV_SE)),
         all_mean = mean(c(SEN, SPE, PPV, NPV)),
         all_error = mean(c(SEN_SE, SPE_SE, PPV_SE, NPV_SE)))

rm(cross_single_norand, cross_single)
```

best predictor
```{r}

```

all single models plot
```{r}
mean_single_melt <- mean_single %>%
  select(-contains("_SE")) %>%
  select(-contains("mean")) %>%
  select(-contains("error")) %>%
  melt(id = "model") %>%
  rename(metric = variable) %>%
  mutate(random_intercepts = case_when(str_detect(model, "study") &
                                         !str_detect(model, "ID") ~ "study",
                                       
                                       str_detect(model, "ID") &
                                         !str_detect(model, "study") ~ "ID",
                                       
                                       str_detect(model, "ID") &
                                         str_detect(model, "study") ~ "study & ID",
                                       
                                       TRUE ~ "no random effects"))

mean_single_error <- mean_single %>%
  select(contains("_SE"), model) %>%
  melt(id = "model") %>%
  rename(metric = variable, error = value) %>%
  mutate(metric = str_remove_all(metric, "_SE")) %>%
  full_join(mean_single_melt)

set.seed(1337)
mean_single_error %>%
  filter(metric != "AUC") %>%
  ggplot(aes(metric, value, color = random_intercepts)) +
  geom_jitter(width = 0.2) +
  theme_bw() +
  labs(title = "Performance of single predictor models")
  #geom_pointrange(aes(ymin = value - error, ymax = value + error), position = "jitter")
```

plot 2
```{r}
mean_single_error_pred <- mean_single_error %>%
  mutate(predictor = case_when(str_detect(model, "range") ~ "range",
                               str_detect(model, "sd") ~ "sd",
                               str_detect(model, "mean") ~ "mean",
                               str_detect(model, "median") ~ "median",
                               str_detect(model, "iqr") ~ "iqr",
                               str_detect(model, "mean_abs") ~ "mean_abs",
                               str_detect(model, "coef_var") ~ "coef_var"))

mean_single_error_pred %>%
  filter(random_intercepts == "study" | random_intercepts == "no random effects") %>%
  ggplot(aes(metric, value, fill = predictor)) +
  geom_bar(stat = "summary", position = "dodge")
```




### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?
this section contains:
- brute force approach (running all combinations of fixed effects trough c-v)


## DIY BRUTE FORCE 
all combinations
```{r}
X = c("mean", "sd", "range", "iqr", "median", "mean_abs", "coef_var", "se", "Gender")

out <- unlist(lapply(1:9, function(n) {
  # get combinations
  combinations <- t(combn(X,n))
  # collapse them into usable formulas:
  formulas <- apply(combinations, 1, 
                    function(row) paste0("diagnosis ~ ", 
                                         paste0(row, collapse = " + "),
                                         paste(" + (1|study)")))}))

out_lrg <- unlist(lapply(1:9, function(n) {
  # get combinations
  combinations <- t(combn(X,n))
  # collapse them into usable formulas:
  formulas <- apply(combinations, 1, 
                    function(row) paste0("diagnosis ~ ", 
                                         paste0(row, collapse = " + "),
                                         paste(" + (1|ID) + (1|study)")))}))

out_norand <- unlist(lapply(1:9, function(n) {
  # get combinations
  combinations <- t(combn(X,n))
  # collapse them into usable formulas:
  formulas <- apply(combinations, 1, 
                    function(row) paste0("diagnosis ~ ", 
                                         paste0(row, collapse = " + ")))}))
```


### the loop
crossvalidates 511 models with (1|study)
output is in the folder
```{r}
set.seed(1337)
cross_out <- map_df(
            #list to loop trough
            out,
            #function to use
            crossvalidation_nation,
            # arguments
            data = Bear_scaled, 
            k = 5,
            dependent = "diagnosis", 
            pos = "1", 
            neg = "0")
```

crossvalidates 511 models with (1|ID) + (1|study)
output is in the folder
```{r}
set.seed(1337)
cross_out_lrg <- map_df(
            #list to loop trough
            out_lrg,
            #function to use
            crossvalidation_nation,
            # arguments
            data = Bear_scaled, 
            k = 5,
            dependent = "diagnosis", 
            pos = "1", 
            neg = "0")
```

crossvalidates 511 models without random effects
output is in the folder
```{r}
set.seed(1337)
cross_out_norand <- map_df(
            #list to loop trough
            out_norand,
            #function to use
            crossvalidation_nation,
            # arguments
            data = Bear_scaled, 
            k = 5,
            dependent = "diagnosis", 
            pos = "1", 
            neg = "0",
            random = FALSE)
```


in case that was run externally
```{r}
cross_out <- read_csv("cross_out.csv")
cross_out_lrg <- read_csv("cross_out_lrg.csv")
cross_out_norand <- read_csv("cross_out_norand.csv")
```


### COMPARISON

```{r}
cross_table <- full_join(cross_out, cross_out_lrg) %>%
  full_join(cross_out_norand)

mean_par <- cross_table %>%
  rowwise() %>%
  mutate(pos_mean = mean(c(SEN, PPV)),
         pos_error = mean(c(SEN_SE, PPV_SE)),
         neg_mean = mean(c(SPE, NPV)),
         neg_error = mean(c(SPE_SE, NPV_SE)),
         all_mean = mean(c(SEN, SPE, PPV, NPV)),
         all_error = mean(c(SEN_SE, SPE_SE, PPV_SE, NPV_SE)))

rm(cross_out, cross_out_lrg, cross_out_norand)
```



### visualizing confusion matrix
nice function
```{r}
confusion_plot <- function(model_name, random = TRUE) {
  
  # TRAINING
  if (isTRUE(random)){

      model <- glmer(model_name, Bear_scaled, family = "binomial")

    } else {
      
      model <- glm(model_name, Bear_scaled, family = "binomial")

    }

  
  # OUTPUT
  mod_df <- Bear_scaled %>%
    ungroup() %>%
    mutate(pred_best_num = inv.logit(predict(model, Bear_scaled, 
                                             allow.new.levels=TRUE)),
           pred_best = ifelse(pred_best_num < 0.5, 0, 1),
           pred_tab = case_when(pred_best == 0 & diagnosis == 0 ~ "true negative",
                                pred_best == 0 & diagnosis == 1 ~ "false negative",
                                pred_best == 1 & diagnosis == 0 ~ "false positive",
                                pred_best == 1 & diagnosis == 1 ~ "true positive"))
  
  mod_summary <- mod_df %>%
    group_by(pred_best) %>%
    summary(n = n())
  
  # PLOT
  plot <- mod_df %>%
    mutate(diagnosis = ifelse(diagnosis == 0, "control", "schizophrenic"),
           pred_best = ifelse(pred_best == 0, "control", "schizophrenic")) %>%
    ggplot(aes(pred_best, pred_best_num, fill = factor(pred_tab))) +
    geom_bar(stat = "identity") +
    theme_bw() +
    theme(panel.grid.major.x = element_blank()) +
    labs(title = "Model performance", 
         subtitle = model_name,
         fill = "truth value", 
         y = "number of model predictions", 
         x = "model prediction")
  
  return(plot)
}

```


#### a few interesting plots
using confusion_plot
```{r}
# most sensitive model
pl_SEN <- confusion_plot("diagnosis ~ coef_var + (1|study)")
pl_SEN

# most specific model
pl_SPE <- confusion_plot("diagnosis ~ mean + range + iqr + (1|study)")
pl_SPE

# highest PPV
pl_PPV <- confusion_plot("diagnosis ~ mean + sd + range + mean_abs + se + Gender + (1|study)")
pl_PPV

# highest NPV
pl_NPV <- confusion_plot("diagnosis ~ mean + median + mean_abs + coef_var + (1|study)")
pl_NPV

# highest accuracy
pl_ACC <- confusion_plot("diagnosis ~ median + se + (1|study)")
pl_ACC


### SUMMARIZED

# highest avg pos SAME AS SEN
pl_pos <- confusion_plot("diagnosis ~ coef_var + (1|study)")
pl_pos

# highest avg neg
pl_neg <- confusion_plot("diagnosis ~ range + iqr + median + se + (1|study)")
pl_neg

# highest mean(all metrics) SAME AS PPV
pl_all <- confusion_plot("diagnosis ~ mean + sd + range + mean_abs + se + Gender + (1|study)")
pl_all 


### BUILDMER MEST
pl_mer <- confusion_plot("diagnosis ~ se + mean + iqr + mean_abs + coef_var + Gender + (1 | study)")
pl_mer
```





### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6

Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.
